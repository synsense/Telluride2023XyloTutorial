{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Live-coding script for Telluride Rockpool / Xylo demo June 2023\n",
    "Dylan Muir / Felix Bauer\n",
    "## Outline\n",
    " 1. How to define and configure an ``LIF`` module containing a spiking neuron\n",
    " 4. How to compose a network\n",
    "    - `Linear` weights\n",
    "    - `Sequential` combinator\n",
    "    - `Residual` combinator\n",
    " 5. Audio task: Spiking Heidelberg Digits\n",
    " 6. Network architecture\n",
    " 7. Training\n",
    " 8. Xylo architecture\n",
    " 9. Mapping, quantization, deployment\n",
    " 10. Inference using ``XyloSim``\n",
    " 11. Inference on Xylo HDK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This live-coding script demonstrates working with Rockpool to train SNN networks for Xylo, on an audio task.\n",
    "\n",
    "First we need to install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Install requirements for this notebook\n",
    "%pip install --quiet rockpool matplotlib torch tonic rich jax jaxlib xylosim samna bitstruct\n",
    "\n",
    "# - Import and configure matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "\n",
    "# - Nice printing\n",
    "from rich import print\n",
    "\n",
    "# - Torch and numpy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# - For displaying images\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rockpool is a deep learning library for SNNs, designed to make it very easy to design, train and deploy applications to neuromorphic hardware.\n",
    "\n",
    "Documentation: https://rockpool.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with a single LIF neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - The LIF module is a Leaky Integrate and Fire spiking neuron\n",
    "from rockpool.nn.modules import LIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Create a single LIF neuron to examine\n",
    "lif = LIF(1, threshold=10.)\n",
    "print(lif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Generate some Poissonian spiking input to the neuron\n",
    "f = 0.02\n",
    "T = 500\n",
    "Nin = 1\n",
    "input_sp = np.random.rand(T, Nin) < f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Evolve the neuron by passing the data through\n",
    "#   `record = True` records and returns internal state\n",
    "out, _, rec_dict = lif(input_sp, record = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - plot the output events\n",
    "plt.plot(out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Let's look at the recorded state. What did we get back from the evolution?\n",
    "rec_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Let's plot the synaptic current `isyn` and membrane potential `vmem`\n",
    "plt.plot(rec_dict['isyn'].squeeze(), label='$I_{syn}$')\n",
    "plt.plot(rec_dict['vmem'].squeeze(), label='$V_{mem}$')\n",
    "plt.plot([0, 500], [10, 10], 'k:', label='threshold')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Rockpool modules all have a `state()` method which returns the internal module state\n",
    "print(lif.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Rockpool modules all have a `parameters()` method which returns the trainable parameters of a module\n",
    "print(lif.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Rockpool modules all have a `simulation_parameters()` method which returns the non-trainable parameters\n",
    "print(lif.simulation_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``tonic`` is a package for managing neuromorphic datasets (https://tonic.readthedocs.io)\n",
    "\n",
    "We'll use tonic to download the Spiking Heidlberg Digits dataset (https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/), and provide a convenient python `torch`-like dataset.\n",
    "\n",
    "``tonic`` also provides data transformations and caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Import tonic, download and import the SHD dataset\n",
    "import tonic\n",
    "train_data = tonic.datasets.SHD('./data')\n",
    "shd_timestep = 1e-6\n",
    "shd_channels = 700\n",
    "shd_classes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Let's examine one sample of the dataset\n",
    "events, label = train_data[1]\n",
    "times = events['t'] * shd_timestep\n",
    "channels = events['x']\n",
    "plt.plot(times, channels, '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - We need to downsample the data to use it (to make the network and training simpler)\n",
    "net_channels = 16\n",
    "net_dt = 10e-3\n",
    "sample_T = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - We'll use `tonic` to downsample the data, using a transformation pipeline\n",
    "\n",
    "import tonic.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    # - Downsample in time and space\n",
    "    T.Downsample(\n",
    "        time_factor=shd_timestep / net_dt,\n",
    "        spatial_factor=net_channels / shd_channels\n",
    "        ),\n",
    "\n",
    "    # - Rasterise the events\n",
    "    T.ToFrame(\n",
    "        sensor_size=(net_channels, 1, 1), time_window=1\n",
    "    ),\n",
    "    \n",
    "    # - Convert to a tensor\n",
    "    torch.Tensor,\n",
    "\n",
    "    # - Make sure the samples are not too long in time\n",
    "    lambda m: torch.squeeze(m)[:sample_T, :],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Reload the dataset with these transformations\n",
    "train_data = tonic.datasets.SHD('./data', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Get one training sample\n",
    "raster, label = train_data[1]\n",
    "\n",
    "# - Extract spike times\n",
    "times, channels = torch.where(raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot this sample\n",
    "plt.plot(times * net_dt, channels, '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a data loader to use in training. This is a standard `torch` data loader, so I'm going to gloss over this cell.\n",
    "\n",
    "We will select only the first 8 class labels to use, since Xylo only supports 8 output channels.\n",
    "\n",
    "We will creata a data loader, using ``tonic`` to provide disk caching of the data. In-memory caching is also supported by ``tonic``, but not used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Create a class which subsets a dataset to a list of matching labels\n",
    "class SubsetClasses(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, match_labels):\n",
    "        indices = []\n",
    "        for idx in range(len(dataset)):\n",
    "            _, label = dataset[idx]\n",
    "            if label in match_labels:\n",
    "                indices.append(idx)\n",
    "\n",
    "        self._subset_ds = torch.utils.data.Subset(dataset, indices)\n",
    "        self._len = len(indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._subset_ds[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define arguments for the data loader\n",
    "dataloader_kwargs = dict(\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=tonic.collation.PadTensors(batch_first=True),\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# - Create the data loader, using `tonic` to provide a disk cache\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    tonic.DiskCachedDataset(\n",
    "        dataset=SubsetClasses(train_data, range(8)),\n",
    "        cache_path=f\"cache/{train_data.__class__.__name__}/train/{net_channels}/{net_dt}\",\n",
    "        reset_cache = False,\n",
    "    ),\n",
    "    **dataloader_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define and train an SNN for the SHD task, to deploy to Xylo. We'll use the ``torch`` backend of Rockpool, which uses the PyTorch automatic differentiation pipeline to train NNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Show an image of the target network architecture\n",
    "Image('images/network-layout-shd.svg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Import the required torch-backed modules and combinators\n",
    "from rockpool.nn.modules import LIFTorch, LinearTorch\n",
    "from rockpool.nn.combinators import Sequential, Residual\n",
    "\n",
    "# - Define a simple network architecture\n",
    "Nin = net_channels\n",
    "Nhid = 20\n",
    "Nout = 8\n",
    "\n",
    "net = Sequential(\n",
    "    LinearTorch((Nin, Nhid)),\n",
    "    LIFTorch(Nhid),\n",
    "\n",
    "    Residual(\n",
    "        LinearTorch((Nhid, Nhid)),\n",
    "        LIFTorch(Nhid),\n",
    "    ),\n",
    "\n",
    "    LinearTorch((Nhid, Nout)),\n",
    "    LIFTorch(Nout),\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - By default all parameters are trainable\n",
    "print(\n",
    "    {\n",
    "        module_name: list(module_parameters.keys())\n",
    "        for module_name, module_parameters in net.parameters().items()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Import the `Constant` decorator, so we can specify non-trainable parameters\n",
    "from rockpool.parameters import Constant\n",
    "\n",
    "# - Define shared neuron parameters to use\n",
    "neuron_parameters = {\n",
    "    'tau_mem': Constant(50e-3),\n",
    "    'tau_syn': Constant(20e-3),\n",
    "    'bias': Constant(0.),\n",
    "    'threshold': Constant(1.),\n",
    "    'dt': net_dt,\n",
    "}\n",
    "\n",
    "# - Define the network with shared parameters\n",
    "net = Sequential(\n",
    "    LinearTorch((Nin, Nhid)),\n",
    "    LIFTorch(Nhid, **neuron_parameters),\n",
    "\n",
    "    Residual(\n",
    "        LinearTorch((Nhid, Nhid)),\n",
    "        LIFTorch(Nhid, **neuron_parameters),\n",
    "    ),\n",
    "\n",
    "    LinearTorch((Nhid, Nout)),\n",
    "    LIFTorch(Nout, **neuron_parameters),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Now only weights are trainable\n",
    "print(\n",
    "    \"Trainable parameters:\",\n",
    "    {\n",
    "        module_name: list(module_parameters.keys())\n",
    "        for module_name, module_parameters in net.parameters().items()\n",
    "    }\n",
    ")\n",
    "print(\n",
    "    \"Non-trainable parameters:\",\n",
    "    {\n",
    "        module_name: list(module_parameters.keys())\n",
    "        for module_name, module_parameters in net.simulation_parameters().items()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Import optimizer and loss function from pytorch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# - Get the optimiser functions\n",
    "optimizer = Adam(net.parameters().astorch(), lr=1e-3)\n",
    "\n",
    "# - Loss function\n",
    "loss_fun = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "\n",
    "if train:\n",
    "    # - Training Loop\n",
    "    num_epochs = 10\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # - Loop over dataset, getting batches\n",
    "        for events, labels in train_dl:\n",
    "            # - Zero the optimizer gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # - Evolve the network with this batch\n",
    "            output, _, _ = net(events)\n",
    "\n",
    "            # - Get the prediction -- number of spikes in each channel\n",
    "            pred = torch.sum(output, dim=1)\n",
    "\n",
    "            # - Get the loss value for this batch\n",
    "            loss = loss_fun(pred, labels)\n",
    "\n",
    "            # - Compute gradients with backward step and update parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # - Print the current loss\n",
    "        print(f'Epoch {e}/{num_epochs}, loss {loss.item():.2e}')\n",
    "else:\n",
    "    # - Load a pre-trained version\n",
    "    net.load('pretrained-37ke.json')\n",
    "\n",
    "    # - Plot the loss curve over training this pre-trained version\n",
    "    Image('loss-pretrained-37ke.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Evolve the trained network over a training sample\n",
    "events, label = train_data[2]\n",
    "out, _, rd = net(events, record = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot the output of the network\n",
    "times, channels = torch.where(out[0])\n",
    "plt.plot(times * net_dt, channels, '|')\n",
    "\n",
    "# - Indicate the target label\n",
    "plt.plot(0.01, label, '>', ms=18)\n",
    "plt.ylim([-1, 8])\n",
    "plt.xlim([0, 1.]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment on hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware specific imports\n",
    "import rockpool.devices.xylo.syns61201 as xylo\n",
    "from rockpool.transform.quantize_methods import channel_quantize\n",
    "\n",
    "# From software model to hardware\n",
    "\n",
    "## Extract computational graph\n",
    "graph = net.as_graph()\n",
    "## Map graph to hardware specifications\n",
    "spec = xylo.mapper(graph)\n",
    "## Quantize parameters\n",
    "spec.update(channel_quantize(**spec))\n",
    "## Generate configuration of the hardware\n",
    "config, is_valid, _ = xylo.config_from_specification(**spec)\n",
    "## Deploy module - for now in precise simulation\n",
    "mod = xylo.XyloSim.from_config(config, dt = net_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Let's look at the mapped weights\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(spec['weights_in'].T)\n",
    "plt.title('$W_{in}$')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(spec['weights_rec'].T)\n",
    "plt.title('$W_{rec}$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(spec['weights_out'].T)\n",
    "plt.title('$W_{out}$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Compare this against our assumed solution from before\n",
    "Image('images/mapped-weights.svg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Let's look at the output of the simulated HDK on a training sample\n",
    "events, label = train_data[2]\n",
    "out_xsim, _, rd_xsim = mod(events.numpy(), record = True)\n",
    "times, channels = np.nonzero(out_xsim)\n",
    "plt.plot(times * net_dt, channels, '|')\n",
    "plt.plot(0.01, label, '>', ms=18)\n",
    "plt.ylim([-0.5, 7.5])\n",
    "plt.xlim([0, 1.]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Let's plot the membrane potential of the output neurons\n",
    "times = np.arange(out_xsim.shape[0]) * net_dt\n",
    "plt.plot(times, rd_xsim['Vmem_out']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Import the helper function to connect to a Xylo HDK\n",
    "from rockpool.devices.xylo import find_xylo_hdks\n",
    "\n",
    "# - Locate an HDK, if one is available\n",
    "hdks, _, _ = find_xylo_hdks()\n",
    "assert len(hdks) > 0, 'The rest of this notebook needs a connected Xylo HDK.'\n",
    "\n",
    "# - We'll use the first connected HDK\n",
    "hdk = hdks[0]\n",
    "\n",
    "# - Now we can create a Rockpool module that wraps the HDK, by providing the configuration bitstream as before\n",
    "mod_hdk = xylo.XyloSamna(hdk, config, net_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Evolve with a single training sample and also record power consumption\n",
    "events, label = train_data[2]\n",
    "out_xhdk, _, rd_xhdk = mod_hdk(events.numpy().astype(int), record = True, record_power=True)\n",
    "\n",
    "print(f'Power measurement: {np.mean(rd_xhdk[\"io_power\"]) * 1e6:.2f} muW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot the output\n",
    "times, channels = np.nonzero(out_xhdk)\n",
    "plt.plot(times * net_dt, channels, '|')\n",
    "plt.plot(0.01, label, '>', ms=18)\n",
    "plt.ylim([-0.5, 7.5])\n",
    "plt.xlim([0, 1.]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Let's compare the output Vmem between the HDK and the simulator\n",
    "times = np.arange(out_xhdk.shape[0]) * net_dt\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(times, rd_xhdk['Vmem_out'])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(times, rd_xsim['Vmem_out']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Xylo-Audio v2 audio front-end interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "\n",
    "The AFE (Audio Front-End) is used to preprocess single-channel audio signals and convert them into spikes.\n",
    "Here the audio signal is input to the AFE by a microphone mounted on the hardware dev kit, or an external differential analog audio signal.\n",
    "The AFE has 16 output channels, and you can adjust its parameters via hyperparameters in the `.AFESamna` class.\n",
    "\n",
    "`.AFESamna` allows you to access the audio front-end on the dev kit, and record encoded audio either from the on-board microphone or analog audio injected to the dev kit.\n",
    "\n",
    "`.AFESamna` also allows a custom config input which is without auto-calibration.\n",
    "If you do not provide a custom config, we highly suggest you set ``auto_calibrate = True`` on instantiation, which helps to mitigate the effects of background and mechanical noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('AFESamna.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Set the time resolution and duration to record encoded audio\n",
    "dt = 10e-3\n",
    "timesteps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Create an AFESamna module, which wraps the AFE on the Xylo A2 HDK\n",
    "mod = xylo.AFESamna(hdk, None, dt=dt, auto_calibrate=True, amplify_level='high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Evolve the module to record encoded real-time audio as events\n",
    "spikes_ts, _, _ = mod(np.zeros([0, timesteps, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot some encoded audio events recorded from the AFE\n",
    "plt.imshow(spikes_ts.T, aspect='auto', interpolation='none')\n",
    "plt.title('#Spikes in AFE output channels')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Channel')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the AFE and SNN cores in free-running inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "\n",
    "Once you have a complete chip HW specification, you can deploy it to the chip in real-time infrence mode, using the class `.XyloMonitor`.\n",
    "This mode uses the AFE core to pre-process audio signals in real time, then send encoded audio to the SNN core for inference.\n",
    "In this mode you only read the output events from the SNN core, without providing input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('XyloMonitor.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Use XyloMonitor to deploy to the HDK\n",
    "# - You need to wait 45s until the AFE auto-calibration is done\n",
    "\n",
    "output_mode = \"Vmem\"\n",
    "amplify_level = \"high\"\n",
    "hibernation = False\n",
    "DN = False\n",
    "T = 10\n",
    "\n",
    "modMonitor = xylo.XyloMonitor(\n",
    "    hdk,\n",
    "    config,\n",
    "    dt=net_dt,\n",
    "    output_mode=output_mode,\n",
    "    amplify_level=amplify_level,\n",
    "    hibernation_mode=hibernation,\n",
    "    divisive_norm=DN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# - A resultList stack to store the results\n",
    "\n",
    "class ResultList(object):\n",
    "    def __init__(self, max_len=100):\n",
    "        self._list = deque(maxlen=max_len)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def reset(self):\n",
    "        self._list = deque(maxlen=max_len)\n",
    "\n",
    "    def append(self, num):\n",
    "        self._list.append(num)\n",
    "\n",
    "    def counts(self, features=None):\n",
    "        features = features or []\n",
    "        count = 0\n",
    "        for _ in self._list:\n",
    "            if _ in features:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._list)\n",
    "\n",
    "    def print_result(self):\n",
    "        return self._list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "# - Draw a real time image for output channels\n",
    "lines = [ResultList(max_len=10) for _ in range(Nout)]\n",
    "time_base = ResultList(max_len=10)\n",
    "tt = 0\n",
    "t_inference = 10.\n",
    "\n",
    "\n",
    "from time import time\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "while (time() - t_start) < t_inference:\n",
    "    # - Perform inference on the Xylo A2 HDK\n",
    "    output, _, _ = modMonitor(input_data=np.zeros((T, Nin)))\n",
    "    if output is not None:\n",
    "        output = np.max(output, axis=0)\n",
    "        for i in range(Nout):\n",
    "            lines[i].append(output[i])\n",
    "\n",
    "        time_base.append(tt)\n",
    "        tt += 0.1\n",
    "        ax_time = time_base.print_result()\n",
    "        \n",
    "        for i in range(Nout):\n",
    "            plt.plot(ax_time, lines[i].print_result(), label=f\"class{i}\")\n",
    "\n",
    "        plt.xlabel('time')\n",
    "        plt.ylabel('Vmem')\n",
    "        plt.legend()\n",
    "        plt.pause(0.1)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "__Talk to Felix, Gregor, or Sadique and start playing with Xylo__\n",
    "\n",
    "Check out further tutorials and documenation on (https://rockpool.ai)!\n",
    "\n",
    "For more information about Xylo™, see (https://rockpool.ai/devices/xylo-overview.html)\n",
    "\n",
    "For information about the Xylo™ HDK, see (https://www.synsense.ai/products/xylo/)\n",
    "\n",
    "For a more in-depth published example, see (https://ieeexplore.ieee.org/document/9967462) (https://doi.org/10.48550/arXiv.2208.12991)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
